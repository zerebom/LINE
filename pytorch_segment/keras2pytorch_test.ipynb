{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitd659ecacd9144e8aa4286e8db0c97f93",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kerasからplへの移行 dataloader編\n",
    "\n",
    "### 必要な機能\n",
    "\n",
    "#### make_path_list\n",
    "- dirから学習pathを取り出す\n",
    "\n",
    "#### dataset\n",
    "input:path_list\n",
    "output:data,\n",
    "- 学習pathからデータをロードして、datatransformを元に変形する\n",
    "- anotationとセットで返す。必要なら、座標もともに返す\n",
    "\n",
    "\n",
    "#### datatransform\n",
    "- 関数を何個か受け取って処理を行う。\n",
    "\n",
    "#### dataloader\n",
    "- dataset,datatransformを合体させて、バッチで返す "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import sys \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/higuchi/ssd/kits19/data'\n",
    "train_patch = 'tumor_48x48x16'\n",
    "val_patch = 'tumor_60x60x20'\n",
    "train_ids=['001','002']\n",
    "val_ids=['001','002']\n",
    "\n",
    "class DataPathMaker():\n",
    "    '''\n",
    "    DataSetに渡すpath_listを作るためのDataFrameを作る\n",
    "    今後の展望として、統計量を持ったDFを渡してその条件でlistを変えるようにする。\n",
    "    '''\n",
    "    # TODO:文字列の除去\n",
    "    def __init__(self, data_dir, patch_dir_name='patch'):\n",
    "        self.data_dir = pathlib.Path(data_dir)\n",
    "        self.patch_dir_name = patch_dir_name\n",
    "        \n",
    "\n",
    "    def create_dataframe(self, id_list):\n",
    "        data = []\n",
    "        for patient_id in id_list:\n",
    "            # TODO: case_00の部分もyamlから渡せるようにしたほうがよい\n",
    "            patient_dir = self.data_dir / f'case_00{patient_id}' / self.patch_dir_name\n",
    "            images = sorted(patient_dir.glob('patch_image_*.npy'))\n",
    "            labels = sorted(patient_dir.glob('patch_no_onehot_*.npy'))\n",
    "            if len(images) == 1 or len(labels) == 0:\n",
    "                print(f'{patient_id} is no data')\n",
    "            for image, label in zip(images, labels):\n",
    "                data.append(['image', patient_id, image])\n",
    "                data.append(['label', patient_id, label])\n",
    "        return pd.DataFrame(data, columns=['type', 'id', 'path'])\n",
    "\n",
    "\n",
    "train_path_df = DataPathMaker(DATA_DIR, patch_dir_name=train_patch).create_dataframe(train_ids)\n",
    "val_path_df = DataPathMaker(DATA_DIR, patch_dir_name=val_patch).create_dataframe(val_ids)\n",
    "\n",
    "train_im_list = train_path_df[train_path_df['type']=='image']['path'].astype(str).values\n",
    "val_im_list = train_path_df[train_path_df['type']=='image']['path'].astype(str).values\n",
    "\n",
    "train_lb_list = train_path_df[train_path_df['type']=='label']['path'].astype(str).values\n",
    "val_lb_list = train_path_df[train_path_df['type']=='label']['path'].astype(str).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"引数transformに格納された変形を順番に実行するクラス\n",
    "       対象画像とアノテーション画像を同時に変換させます。 \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, anno_class_img):\n",
    "        for t in self.transforms:\n",
    "            img, anno_class_img = t(img, anno_class_img)\n",
    "        return img, anno_class_img\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    def __init__(self, scale):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, img, anno_class_img):\n",
    "\n",
    "        width = img.size[0]  # img.size=[幅][高さ]\n",
    "        height = img.size[1]  # img.size=[幅][高さ]\n",
    "\n",
    "        # 拡大倍率をランダムに設定\n",
    "        scale = np.random.uniform(self.scale[0], self.scale[1])\n",
    "\n",
    "        scaled_w = int(width * scale)  # img.size=[幅][高さ]\n",
    "        scaled_h = int(height * scale)  # img.size=[幅][高さ]\n",
    "\n",
    "        # 画像のリサイズ\n",
    "        img = img.resize((scaled_w, scaled_h), Image.BICUBIC)\n",
    "\n",
    "        # アノテーションのリサイズ\n",
    "        anno_class_img = anno_class_img.resize(\n",
    "            (scaled_w, scaled_h), Image.NEAREST)\n",
    "\n",
    "        # 画像を元の大きさに\n",
    "        # 切り出し位置を求める\n",
    "        if scale > 1.0:\n",
    "            left = scaled_w - width\n",
    "            left = int(np.random.uniform(0, left))\n",
    "\n",
    "            top = scaled_h-height\n",
    "            top = int(np.random.uniform(0, top))\n",
    "\n",
    "            img = img.crop((left, top, left+width, top+height))\n",
    "            anno_class_img = anno_class_img.crop(\n",
    "                (left, top, left+width, top+height))\n",
    "\n",
    "        else:\n",
    "            # input_sizeよりも短い辺はpaddingする\n",
    "            p_palette = anno_class_img.copy().getpalette()\n",
    "\n",
    "            img_original = img.copy()\n",
    "            anno_class_img_original = anno_class_img.copy()\n",
    "\n",
    "            pad_width = width-scaled_w\n",
    "            pad_width_left = int(np.random.uniform(0, pad_width))\n",
    "\n",
    "            pad_height = height-scaled_h\n",
    "            pad_height_top = int(np.random.uniform(0, pad_height))\n",
    "\n",
    "            img = Image.new(img.mode, (width, height), (0, 0, 0))\n",
    "            img.paste(img_original, (pad_width_left, pad_height_top))\n",
    "\n",
    "            anno_class_img = Image.new(\n",
    "                anno_class_img.mode, (width, height), (0))\n",
    "            anno_class_img.paste(anno_class_img_original,\n",
    "                                 (pad_width_left, pad_height_top))\n",
    "            anno_class_img.putpalette(p_palette)\n",
    "\n",
    "        return img, anno_class_img\n",
    "\n",
    "\n",
    "class RandomRotation(object):\n",
    "    def __init__(self, angle):\n",
    "        self.angle = angle\n",
    "\n",
    "    def __call__(self, img, anno_class_img):\n",
    "\n",
    "        # 回転角度を決める\n",
    "        rotate_angle = (np.random.uniform(self.angle[0], self.angle[1]))\n",
    "\n",
    "        # 回転\n",
    "        img = img.rotate(rotate_angle, Image.BILINEAR)\n",
    "        anno_class_img = anno_class_img.rotate(rotate_angle, Image.NEAREST)\n",
    "\n",
    "        return img, anno_class_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(16, 3, 48, 48)\n"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "class KitsDataSet(data.Dataset):\n",
    "    '''\n",
    "    loadした後のデータの処理のみを行う。\n",
    "    input:train,val,testのdata_list(絞り込み済み) & label_list\n",
    "    '''\n",
    "    # TODO: ラベルが重ねってる部分の処理(binaly_labels)\n",
    "    # TODO: Augment_code\n",
    "    def __init__(self, img_list, label_list, transform, phase='train'):\n",
    "        self.img_list = img_list\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        im = np.load(self.img_list[index])\n",
    "        lb = np.load(self.label_list[index])\n",
    "    \n",
    "        if self.transform:\n",
    "            im, lb = self.transform(im, lb, self.phase)\n",
    "\n",
    "        # B,D,C,H,Wに変換する\n",
    "        # im = im.permute(3, 2, 0, 1)\n",
    "        im = np.transpose(im, (2, 3, 0, 1))\n",
    "\n",
    "        # B,C,H,Wに変換する\n",
    "        # lb = lb.permute(2, 0, 1)\n",
    "\n",
    "        #numpy ver\n",
    "        lb = to_categorical(lb, num_classes=3)\n",
    "        lb = np.transpose(lb, (2, 3, 0, 1))\n",
    "        return im, lb \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tr_DS = KitsDataSet(train_im_list,train_lb_list,phase='train', transform = None)\n",
    "val_DS = KitsDataSet(val_im_list,val_lb_list,phase='val', transform = None)\n",
    "\n",
    "\n",
    "print(tr_DS.__getitem__(0)[1].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([8, 16, 1, 48, 48])\ntorch.Size([8, 16, 3, 48, 48])\n"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_dl = data.DataLoader(tr_DS, batch_size = batch_size)\n",
    "val_dl = data.DataLoader(val_DS, batch_size = batch_size)\n",
    "\n",
    "dataloaders_dict = {\n",
    "    \"train\": train_dl,\n",
    "    \"val\": val_dl\n",
    "}\n",
    "\n",
    "batch_iterator = iter(dataloaders_dict[\"val\"])\n",
    "\n",
    "images, labels = next(batch_iterator)\n",
    "\n",
    "print(images.size())\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "grid = torchvision.utils.make_grid\\\n",
    "(images[:,0,:, :].to(torch.int16).abs())\n",
    "\n",
    "# grid = transforms.ToPILImage()(grid)grid\n",
    "\n",
    "plt.imshow(grid.permute(1,2,0).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self,input_shape,nclasses):\n",
    "    \n",
    "\n",
    "class SingleConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation='relu'):\n",
    "        super(SingleConv).__init__()\n",
    "\n",
    "        for name, module in create_conv(in_channels, out_channels, kernel_size, order, padding=padding):\n",
    "            self.add_module(name, module)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation='relu'):\n",
    "        super(DoubleConv).__init__()\n",
    "\n",
    "        self.add_module('SingleConv1', SingleConv(in_channels, out_channels, kernel_size, order, activation))\n",
    "        self.add_module('SingleConv1', SingleConv(in_channels, out_channels, kernel_size, order, activation))\n",
    "\n",
    "\n",
    "def create_conv(in_channels, out_channels, kernel_size, activation, padding=1):\n",
    "    modules = []\n",
    "    modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
    "    modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n",
    "    modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
    "    return modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=<__main__.DataTrasform object at 0x7fe25f838710>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0ae5a79a66ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mval_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# auto_collation without custom batch_sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mbatch_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             raise ValueError(\"batch_size should be a positive integer value, \"\n\u001b[0;32m--> 190\u001b[0;31m                              \"but got batch_size={}\".format(batch_size))\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             raise ValueError(\"drop_last should be a boolean value, but got \"\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=<__main__.DataTrasform object at 0x7fe25f838710>"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class KitsDataSet(data.Dataset):\n",
    "    def __init__(self, data_dir, patch_dir_name, phase='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.patch_dir_name = patch_dir_name\n",
    "        self.phase = phase\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __get\n",
    "    \n",
    "\n",
    "\n",
    "train_ds = KitsDataSet()\n",
    "val_ds = KitsDataSet()\n",
    "\n",
    "train_tf = DataTrasform()\n",
    "val_tf = DataTrasform()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}